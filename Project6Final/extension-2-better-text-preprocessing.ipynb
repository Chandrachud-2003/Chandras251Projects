{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension 2 - Better text preprocessing\n",
    "\n",
    "- If you look at the top words extracted from the email dataset, many of them are common \"stop words\" (e.g. a, the, to, etc.) that do not carry much meaning when it comes to differentiating between spam vs. non-spam email. Improve your preprocessing pipeline by building your top words without stop words. Analyze performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Importing all the required stuff\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/chandrachudgowda/.pyenv/versions/3.10.4/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/chandrachudgowda/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/chandrachudgowda/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: click in /Users/chandrachudgowda/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/chandrachudgowda/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email_preprocessor as epp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq, num_emails = epp.count_words(remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You found 32625 emails in the datset. You should have found 32625.\n"
     ]
    }
   ],
   "source": [
    "print(f'You found {num_emails} emails in the datset. You should have found 32625.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "1. enron: 60852\n",
      "2. subject: 46443\n",
      "3. ect: 35346\n",
      "4. com: 22742\n",
      "5. company: 21296\n",
      "6. please: 19490\n",
      "7. hou: 17264\n",
      "8. would: 15166\n",
      "9. e: 14756\n",
      "10. new: 14729\n"
     ]
    }
   ],
   "source": [
    "top_words, top_counts = epp.find_top_words(word_freq)\n",
    "\n",
    "# Printing the top 10 words\n",
    "print('Top 10 words:')\n",
    "for i in range(10):\n",
    "    print(f'{i + 1}. {top_words[i]}: {top_counts[i]}')\n",
    "\n",
    "# Checking if the top words does not contain any stop words\n",
    "assert 'the' not in top_words\n",
    "assert 'to' not in top_words\n",
    "assert 'and' not in top_words\n",
    "assert 'of' not in top_words\n",
    "assert 'a' not in top_words\n",
    "assert 'in' not in top_words\n",
    "assert 'for' not in top_words\n",
    "assert 'is' not in top_words\n",
    "assert 'on' not in top_words\n",
    "assert 'that' not in top_words\n",
    "assert 'this' not in top_words\n",
    "assert 'it' not in top_words\n",
    "assert 'you' not in top_words\n",
    "assert 'not' not in top_words\n",
    "assert 'are' not in top_words\n",
    "assert 'be' not in top_words\n",
    "assert 'have' not in top_words\n",
    "assert 'as' not in top_words\n",
    "assert 'with' not in top_words\n",
    "assert 'will' not in top_words\n",
    "assert 'at' not in top_words\n",
    "assert 'by' not in top_words\n",
    "assert 'from' not in top_words\n",
    "assert 'or' not in top_words\n",
    "assert 'an' not in top_words\n",
    "assert 'was' not in top_words\n",
    "assert 'if' not in top_words\n",
    "assert 'they' not in top_words\n",
    "assert 'but' not in top_words\n",
    "assert 'your' not in top_words\n",
    "assert 'we' not in top_words\n",
    "assert 'all' not in top_words\n",
    "assert 'can' not in top_words\n",
    "assert 'more' not in top_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report + Results\n",
    "\n",
    "For this extension, I aimed to improve the text preprocessing pipeline by removing stop words from the dataset. I first imported the required libraries and then installed the nltk library to help with the stop word removal. Next, I used the count_words function from the original code to count the frequency of words in the dataset with the remove_stop_words parameter set to True. I found that there were 32,625 emails in the dataset.\n",
    "\n",
    "After counting the words, I used the find_top_words function to find the top 10 most frequently used words in the dataset. These words were:\n",
    "\n",
    "enron: 60852\n",
    "subject: 46443\n",
    "ect: 35346\n",
    "com: 22742\n",
    "company: 21296\n",
    "please: 19490\n",
    "hou: 17264\n",
    "would: 15166\n",
    "e: 14756\n",
    "new: 14729\n",
    "I then verified that the top words did not contain any stop words, as per the requirements. I asserted that the words \"the\", \"to\", \"and\", \"of\", \"a\", \"in\", \"for\", \"is\", \"on\", \"that\", \"this\", \"it\", \"you\", \"not\", \"are\", \"be\", \"have\", \"as\", \"with\", \"will\", \"at\", \"by\", \"from\", \"or\", \"an\", \"was\", \"if\", \"they\", \"but\", \"your\", \"we\", and \"all\" were not present in the top words.\n",
    "\n",
    "In order to remove stop words, I modified the tokenize_words function to remove stop words if the remove_stop_words parameter was set to True. I then used the count_words function with remove_stop_words set to True to count the frequency of the non-stop words in the dataset.\n",
    "\n",
    "Here are the original results:\n",
    "\n",
    "Your top 5 words are\n",
    "['the', 'to', 'and', 'of', 'a']\n",
    "with counts of\n",
    "[277459, 203659, 148873, 139578, 111841].\n",
    "These results are consistent with what we would expect from a large corpus of text.\n",
    "\n",
    "However, when we remove stop words, the top 10 words are different. Here are the new results:\n",
    "\n",
    "Top 10 words:\n",
    "\n",
    "enron: 60852\n",
    "subject: 46443\n",
    "ect: 35346\n",
    "com: 22742\n",
    "company: 21296\n",
    "please: 19490\n",
    "hou: 17264\n",
    "would: 15166\n",
    "e: 14756\n",
    "new: 14729\n",
    "\n",
    "As we can see, the most frequent words now include terms that are specific to the context of the text corpus. This suggests that removing stop words can help us to identify the most meaningful words in a given text.\n",
    "\n",
    "It is worth noting that removing stop words may still have other benefits, such as reducing the size of the vocabulary and potentially improving model performance in other ways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
