{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature size\n",
    "\n",
    "- Explore how the number of selected features for the email dataset influences accuracy and runtime performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chandrachudgowda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import email_preprocessor as epp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set the number of features to use for the experiment\n",
    "num_features = [50, 100, 150, 200]\n",
    "\n",
    "# Count the words in the dataset\n",
    "word_freq, num_emails = epp.count_words()\n",
    "\n",
    "# Find the top words based on word frequency\n",
    "top_words, top_counts = epp.find_top_words(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50\n",
      "Accuracy: 0.8521072796934865\n",
      "Runtime: 4.2484s\n",
      "\n",
      "Number of features: 100\n",
      "Accuracy: 0.8908812260536398\n",
      "Runtime: 7.5903s\n",
      "\n",
      "Number of features: 150\n",
      "Accuracy: 0.9262835249042145\n",
      "Runtime: 8.4006s\n",
      "\n",
      "Number of features: 200\n",
      "Accuracy: 0.9459003831417625\n",
      "Runtime: 9.5856s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in num_features:\n",
    "    # Select the top `n` words\n",
    "    features = top_words[:n]\n",
    "\n",
    "    # Convert emails to feature vectors\n",
    "    feature_vectors, y = epp.make_feature_vectors(features, num_emails)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    np.random.seed(0)\n",
    "    x_train, y_train, inds_train, x_test, y_test, inds_test = epp.make_train_test_sets(feature_vectors, y)\n",
    "\n",
    "    # Save the data\n",
    "    np.save(f'data/email_train_x_{n}.npy', x_train)\n",
    "    np.save(f'data/email_train_y_{n}.npy', y_train)\n",
    "    np.save(f'data/email_train_inds_{n}.npy', inds_train)\n",
    "    np.save(f'data/email_test_x_{n}.npy', x_test)\n",
    "    np.save(f'data/email_test_y_{n}.npy', y_test)\n",
    "    np.save(f'data/email_test_inds_{n}.npy', inds_test)\n",
    "\n",
    "    # Train a logistic regression model on the training data\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    start_time = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Predict on the test data and compute accuracy\n",
    "    y_pred = clf.predict(x_test)\n",
    "    acc = np.mean(y_pred == y_test)\n",
    "\n",
    "    # Print results\n",
    "    print(f'Number of features: {n}')\n",
    "    print(f'Accuracy: {acc}')\n",
    "    print(f'Runtime: {end_time - start_time:.4f}s\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report + Results\n",
    "\n",
    "For my extension, I explored how the number of selected features for the email dataset influences accuracy and runtime performance. To do this, I used the existing code and added a loop that iterates over a range of feature sizes. The range of feature sizes I used was [50, 100, 150, 200]. For each iteration, I selected the top 'n' words from the email dataset, where 'n' is the feature size for that iteration. Then, I converted the emails to feature vectors using the selected features and split the data into training and test sets. Next, I trained a logistic regression model on the training data and predicted on the test data to compute accuracy. Finally, I saved the training and test data for each feature size to files.\n",
    "\n",
    "The results of my extension are as follows:\n",
    "\n",
    "For 50 features, the accuracy was 0.852 and the runtime was 4.248 seconds.\n",
    "For 100 features, the accuracy was 0.891 and the runtime was 7.590 seconds.\n",
    "For 150 features, the accuracy was 0.926 and the runtime was 8.401 seconds.\n",
    "For 200 features, the accuracy was 0.946 and the runtime was 9.586 seconds.\n",
    "\n",
    "As the number of features increased, the accuracy of the model increased. This makes sense as more features allow the model to better capture the nuances of the data. However, as the number of features increased, so did the runtime of the model. This is because more features require more computation to train and predict. Therefore, there is a trade-off between accuracy and runtime when selecting the number of features for a model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
